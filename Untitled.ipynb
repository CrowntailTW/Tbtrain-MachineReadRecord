{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Google 提供六種預訓練模型\\n\n",
    "bert-base-uncased: 12-layer, 768-hidden, 12-heads, 110M parameters  \n",
    "bert-large-uncased: 24-layer, 1024-hidden, 16-heads, 340M parameters  \n",
    "bert-base-cased: 12-layer, 768-hidden, 12-heads , 110M parameters  \n",
    "bert-base-multilingual: 102 languages, 12-layer, 768-hidden, 12-heads, 110M parameters  \n",
    "bert-base-chinese: Chinese Simplified and Traditional, 12-layer, 768-hidden, 12-heads, 110M parameters  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fer : https://www.kaggle.com/sushanth1995/text-classification-with-bert-xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install bert-for-tf2\n",
    "# !pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# https://www.kaggle.com/sushanth1995/text-classification-with-bert-xgboost\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score\n",
    "\n",
    "# from keras.preprocessing.text import Tokenizer\n",
    "# from keras.preprocessing.sequence import pad_sequences\n",
    "# from tqdm import tqdm\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Embedding,LSTM,Dense,SpatialDropout1D\n",
    "# from keras.initializers import Constant\n",
    "# from keras.optimizers import Adam\n",
    "# from keras import regularizers\n",
    "# import keras.backend as K\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "try:\n",
    "    %tensorflow_version 2.x\n",
    "except Exception:\n",
    "    pass\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "import bert\n",
    "\n",
    "# for ploty\n",
    "import cufflinks as cf\n",
    "cf.go_offline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-5-cb50da41978a>:3: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n",
      "***If TF can access GPU: ***\n",
      "\n",
      " False\n"
     ]
    }
   ],
   "source": [
    "value = tf.test.is_gpu_available(\n",
    "    cuda_only=False,\n",
    "    min_cuda_compute_capability=None\n",
    ")\n",
    "print ('***If TF can access GPU: ***\\n\\n',value) # MUST RETURN True IF IT CAN!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf \n",
    "# device_name = tf.test.gpu_device_name()\n",
    "# if device_name != '/device:GPU:0':\n",
    "#     raise SystemError('GPU device not found')\n",
    "# print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cores = 4\n",
    "num_CPU = 1\n",
    "num_GPU = 0\n",
    "config = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=num_cores,\\\n",
    "        inter_op_parallelism_threads=num_cores, allow_soft_placement=True,\\\n",
    "        device_count = {'CPU' : num_CPU, 'GPU' : num_GPU})\n",
    "session = tf.compat.v1.Session(config=config)\n",
    "from tensorflow.python.keras import backend as K2\n",
    "K2.set_session(session)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train= pd.read_csv('./data/trainset.csv')\n",
    "# train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test=pd.read_csv('./data/testset.csv')\n",
    "# test.set_index(test['Id'],inplace=True)\n",
    "# test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # remove punctuation\n",
    "# import string\n",
    "# input_str = \"This &is [an] example? {of} string. with.? punctuation!!!!\" # Sample string\n",
    "# PR = lambda s:s.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "# train['Abstract_proc'] = train.Abstract.apply(PR).apply(str.lower)\n",
    "# test['Abstract_proc'] = test.Abstract.apply(PR).apply(str.lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # !pip install nltk\n",
    "# # !pip install spacy\n",
    "# # remove stop word\n",
    "# import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk_stopwords = nltk.corpus.stopwords.words('english')\n",
    "# print('NLTK has {} stop words'.format(len(nltk_stopwords)))\n",
    "# print('The first five stop words are {}'.format(list(nltk_stopwords)[:5]))\n",
    "\n",
    "# import spacy\n",
    "# nlp = spacy.load('en_core_web_sm')\n",
    "# spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "# print('spaCy has {} stop words'.format(len(spacy_stopwords)))\n",
    "# print('The first five stop words are {}'.format(list(spacy_stopwords)[:5]))\n",
    "\n",
    "\n",
    "\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# def rmsw_function(text, stopword_list):\n",
    "#     return ' '.join([word for word in word_tokenize(text) if word not in stopword_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train['Abstract_proc'] = train['Abstract_proc'].apply(lambda x: rmsw_function(x, spacy_stopwords)).apply(lambda x: rmsw_function(x, nltk_stopwords))\n",
    "# test['Abstract_proc']  = test['Abstract_proc'].apply(lambda x: rmsw_function(x, spacy_stopwords)).apply(lambda x: rmsw_function(x, nltk_stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # remove number\n",
    "# import re\n",
    "# def removeNumber(text):\n",
    "#     remove_chars = '[0-9]'\n",
    "#     return re.sub(remove_chars, '', text)\n",
    "# train['Abstract_proc'] = train['Abstract_proc'].apply(removeNumber)\n",
    "# test['Abstract_proc']  = test['Abstract_proc'].apply(removeNumber)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for c in set([x for s in train['Classifications'] for x in s.split(' ')]):\n",
    "#     train[c]   = [1 if c   in ele else 0 for ele in train['Classifications']]\n",
    "# #     train[c].value_counts().iplot(kind='bar',title=c, color=['red'])\n",
    "# train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.to_csv('./data/trainset_proc.csv',index=False)\n",
    "# test.to_csv('./data/testset_proc.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train= pd.read_csv('./data/trainset_proc.csv')\n",
    "test= pd.read_csv('./data/testset_proc.csv')\n",
    "test.set_index(test['Id'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>Classifications</th>\n",
       "      <th>Abstract_proc</th>\n",
       "      <th>EMPIRICAL</th>\n",
       "      <th>ENGINEERING</th>\n",
       "      <th>THEORETICAL</th>\n",
       "      <th>OTHERS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Distinctiveness, complexity, and repeatability...</td>\n",
       "      <td>This paper proposes three measures to quantify...</td>\n",
       "      <td>ENGINEERING</td>\n",
       "      <td>paper proposes measures quantify characteristi...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>An Adaptable Fast Matrix Multiplication Algori...</td>\n",
       "      <td>In this paper we present an adaptable fast mat...</td>\n",
       "      <td>EMPIRICAL</td>\n",
       "      <td>paper present adaptable fast matrix multiplica...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Trimming the Multipath for Efficient Dynamic R...</td>\n",
       "      <td>Multipath routing is a trivial way to exploit ...</td>\n",
       "      <td>THEORETICAL ENGINEERING</td>\n",
       "      <td>multipath routing trivial way exploit path div...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Learning Word Embeddings from Speech</td>\n",
       "      <td>In this paper, we propose a novel deep neural ...</td>\n",
       "      <td>ENGINEERING EMPIRICAL</td>\n",
       "      <td>paper propose novel deep neural network archit...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A Unified Model for Near and Remote Sensing</td>\n",
       "      <td>We propose a novel convolutional neural networ...</td>\n",
       "      <td>THEORETICAL ENGINEERING</td>\n",
       "      <td>propose novel convolutional neural network arc...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "0  Distinctiveness, complexity, and repeatability...   \n",
       "1  An Adaptable Fast Matrix Multiplication Algori...   \n",
       "2  Trimming the Multipath for Efficient Dynamic R...   \n",
       "3               Learning Word Embeddings from Speech   \n",
       "4        A Unified Model for Near and Remote Sensing   \n",
       "\n",
       "                                            Abstract          Classifications  \\\n",
       "0  This paper proposes three measures to quantify...              ENGINEERING   \n",
       "1  In this paper we present an adaptable fast mat...                EMPIRICAL   \n",
       "2  Multipath routing is a trivial way to exploit ...  THEORETICAL ENGINEERING   \n",
       "3  In this paper, we propose a novel deep neural ...    ENGINEERING EMPIRICAL   \n",
       "4  We propose a novel convolutional neural networ...  THEORETICAL ENGINEERING   \n",
       "\n",
       "                                       Abstract_proc  EMPIRICAL  ENGINEERING  \\\n",
       "0  paper proposes measures quantify characteristi...          0            1   \n",
       "1  paper present adaptable fast matrix multiplica...          1            0   \n",
       "2  multipath routing trivial way exploit path div...          0            1   \n",
       "3  paper propose novel deep neural network archit...          1            1   \n",
       "4  propose novel convolutional neural network arc...          0            1   \n",
       "\n",
       "   THEORETICAL  OTHERS  \n",
       "0            0       0  \n",
       "1            0       0  \n",
       "2            1       0  \n",
       "3            0       0  \n",
       "4            1       0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(set(train.loc[train.OTHERS==1,'Classifications'] )) ==1: print('!!!! OTRHER CLASSIFICATION IS UNIQUE')\n",
    "print('and',train.loc[train.OTHERS==1,'Classifications'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.displot(np.array([len(s) for s in train.Abstract_proc]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用 BERT 進行微調  \n",
    "微調示例使用了 BERT-Base，它應該能夠使用給定的超引數在配備至少 12GB RAM 的 GPU 上執行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokens : word\n",
    "\n",
    "# mask\n",
    "\n",
    "#segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Abstract_proc'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"BERT Methods Predefined\"\"\"\n",
    "def bert_encode(texts, tokenizer, max_len=max_len):\n",
    "    all_tokens = []\n",
    "    all_masks = []\n",
    "    all_segments = []\n",
    "    \n",
    "    for text in texts:\n",
    "        text = tokenizer.tokenize(text)\n",
    "            \n",
    "        text = text[:max_len-2]\n",
    "        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n",
    "        pad_len = max_len - len(input_sequence)\n",
    "        \n",
    "        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n",
    "        tokens += [0] * pad_len\n",
    "        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n",
    "        segment_ids = [0] * max_len\n",
    "        \n",
    "        all_tokens.append(tokens)\n",
    "        all_masks.append(pad_masks)\n",
    "        all_segments.append(segment_ids)\n",
    "    \n",
    "    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('There are {} rows and {} columns in train'.format(train.shape[0],train.shape[1]))\n",
    "print('There are {} rows and {} columns in train'.format(test.shape[0],test.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/1\"\n",
    "# module_url = \"https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1\"\n",
    "# module_url = r\"./bert/small_bert_bert_en_uncased_L-4_H-512_A-8_1.tar.gz\"\n",
    "bert_layer = hub.KerasLayer(module_url, trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BertTokenizer = bert.bert_tokenization.FullTokenizer\n",
    "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = BertTokenizer(vocab_file, do_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_input = bert_encode(train['Abstract'].values, tokenizer, max_len=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_labels = train['Classifications'].values.copy()\n",
    "full_labels = train[['EMPIRICAL', 'ENGINEERING', 'THEORETICAL', 'OTHERS']].values.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(full_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_input[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data, train_labels, val_labels = train_test_split(train['Abstract'].values,\n",
    "                                                                  full_labels,\n",
    "                                                                  test_size=0.15, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input = bert_encode(train_data, tokenizer, max_len=max_len)\n",
    "val_input = bert_encode(val_data, tokenizer, max_len=max_len)\n",
    "test_input = bert_encode(test['Abstract'].values, tokenizer, max_len=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Bert model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=1e-5\n",
    "decay=5e-5\n",
    "max_len=max_len\n",
    "Dropout_num = 0.25\n",
    "# lr_schedule = [5e-8,1e-8,5e-9,1e-9]\n",
    "# lr_schedule = [1e-6, 5e-7, 1e-7, 5e-8, 1e-8, 1e-9]\n",
    "lr_schedule = [9e-7,1e-8,5e-8,9e-8,7e-9,1e-9]\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n",
    "segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n",
    "pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
    "clf_output = sequence_output[:, 0, :]\n",
    "# clf_pooled = pooled_output[:, 0, :]\n",
    "\n",
    "## Type1\n",
    "# out = Dense(100, activation='relu', activity_regularizer=regularizers.l2(9e-5))(clf_output)\n",
    "# out = Dense(100, activation='relu', activity_regularizer=regularizers.l2(9e-5))(out)\n",
    "# out = Dense(100, activation='relu')(out)\n",
    "\n",
    "## Type2\n",
    "out = Dropout(Dropout_num)(clf_output)\n",
    "out = Dense(32, activation='relu', activity_regularizer=regularizers.l2(1e-5))(clf_output)\n",
    "out = Dense(16, activation='relu', activity_regularizer=regularizers.l2(1e-5))(out)\n",
    "# out = Dense(1024, activation='relu', activity_regularizer=regularizers.l2(1e-5))(out)\n",
    "# out = Dense(1024, activation='relu', activity_regularizer=regularizers.l2(1e-5))(out)\n",
    "\n",
    "## Type3\n",
    "# out = clf_output\n",
    "\n",
    "num_classes = full_labels.shape[1]#len(set(full_labels))\n",
    "out = Dense(num_classes, activation='sigmoid')(out)\n",
    "\n",
    "\n",
    "## Type4\n",
    "# out = clf_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sBERT = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n",
    "# sBERT.compile(SGD(lr=learning_rate, momentum=0.9), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "sBERT.compile(Adam(lr=learning_rate, decay=decay), loss='binary_crossentropy', metrics=['accuracy',f1_m])#CategoricalCrossentropy\n",
    "sBERT.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_weights = sBERT.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint1 = ModelCheckpoint('./model/best_val_f1_m.h5',\n",
    "                             monitor='val_f1_m',\n",
    "                             mode='max',\n",
    "                             save_best_only=True)\n",
    "# checkpoint2 = ModelCheckpoint('best_loss.h5',\n",
    "#                              monitor='val_loss',\n",
    "#                              save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train_input, val_input,test_input, train_labels, val_labels\n",
    "# full_labels\n",
    "\n",
    "train_history = sBERT.fit(\n",
    "    train_input, np.array(train_labels),\n",
    "    epochs = 5,\n",
    "    batch_size = 32,\n",
    "    validation_data=(val_input, np.array(val_labels)),\n",
    "    callbacks=[checkpoint1]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sBERT.save_weights('best_accuracy.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = sBERT.predict((test_input[0][:2],test_input[1][:2],test_input[2][:2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = sBERT.predict(test_input)\n",
    "# print(\" - test_f1_score: {}\".format(f1_score(test_labels,test_pred.round())))\n",
    "# print()\n",
    "\n",
    "# sBERT.save_weights('best_accuracy.h5')\n",
    "# test_accuracy = f1_score(test_labels,test_pred.round())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.set_value(sBERT.optimizer.lr, 1e-6)\n",
    "sBERT.fit(\n",
    "    full_input, full_labels,\n",
    "    epochs = 1,\n",
    "    batch_size = 8\n",
    ")\n",
    "test_pred = sBERT.predict(test_input)\n",
    "epoch_test_accuracy = f1_score(test_labels,test_pred.round())\n",
    "print(\" - test_f1_score: {}\".format(epoch_test_accuracy))\n",
    "print()\n",
    "\n",
    "if epoch_test_accuracy >= test_accuracy:\n",
    "    sBERT.save('best_accuracy.h5')\n",
    "    test_accuracy = epoch_test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.set_value(sBERT.optimizer.lr, 1e-7)\n",
    "sBERT.fit(\n",
    "    full_input, full_labels,\n",
    "    epochs = 1,\n",
    "    batch_size = 16\n",
    ")\n",
    "test_pred = sBERT.predict(test_input)\n",
    "epoch_test_accuracy = f1_score(test_labels,test_pred.round())\n",
    "print(\" - test_f1_score: {}\".format(epoch_test_accuracy))\n",
    "print()\n",
    "\n",
    "if epoch_test_accuracy >= test_accuracy:\n",
    "    sBERT.save('best_accuracy.h5')\n",
    "    test_accuracy = epoch_test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sBERT.layers[3].trainable = False\n",
    "sBERT.compile(Adam(lr=1e-6, decay=1e-6), loss='binary_crossentropy', metrics=['accuracy',f1_m])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
